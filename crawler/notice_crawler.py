# í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì˜ ì½”ë“œ
# (cmd -> $ cd ./crawler -> $ python notice_crawler.py)

# import time
# from urllib.parse import urljoin
# from bs4 import BeautifulSoup
# from selenium import webdriver
# from selenium.webdriver.chrome.service import Service
# from webdriver_manager.chrome import ChromeDriverManager

# # 1. Selenium ë“œë¼ì´ë²„ ì‹¤í–‰
# options = webdriver.ChromeOptions()
# options.add_argument("--headless")  # ì°½ ì—†ì´ ì‹¤í–‰í•˜ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ í•´ì œ
# # options.add_argument("--no-sandbox")
# # options.add_argument("--disable-dev-shm-usage")


# # í¬ë¡¬ ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜ ë° ì‹¤í–‰
# # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
# driver = webdriver.Chrome(options=options)

# # í˜ì´ì§€ ìˆ˜ ì„¤ì •
# MAX_PAGE = 10  # ì‹¤ì œ í•„ìš”í•œ ë§Œí¼ ì¡°ì •

# # 2. ì ‘ì†í•  URL (&page=)
# BASE_URL = "https://eie.chosun.ac.kr"
# LIST_URL = "https://eie.chosun.ac.kr/eie/5563/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGZWllJTJGMzIwJTJGYXJ0Y2xMaXN0LmRvJTNG"

# all_data = []

# for page in range(1, MAX_PAGE + 1):
#     print(f"â–¶ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

#     # í˜ì´ì§€ ì´ë™ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° êµ¬ì„±
#     url = f"{LIST_URL}&amp=&page={page}"
#     driver.get(url)

#     # 3. í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
#     time.sleep(2)

#     # 4. BeautifulSoupë¡œ íŒŒì‹±
#     soup = BeautifulSoup(driver.page_source, 'html.parser')

#     # 5. ê³µì§€ì‚¬í•­ í–‰ ì „ì²´ ê°€ì ¸ì˜¤ê¸° (ìƒë‹¨ê³ ì • í¬í•¨ ì¼ë°˜ê³µì§€ê¹Œì§€ ëª¨ë‘)
#     # rows = soup.select('table.board-table.horizon5 tbody tr')
#     # 5. ê³µì§€ì‚¬í•­ í–‰ ì „ì²´ ê°€ì ¸ì˜¤ê¸° (ìƒë‹¨ê³ ì • ì œì™¸í•œ ì¼ë°˜ê³µì§€ë§Œ)
#     rows = soup.select("table.board-table.horizon5 tbody tr:not(.notice)")

#     print(f"í¬ë¡¤ë§í•œ ê³µì§€ ìˆ˜: {len(rows)}")

#     # 6. ê³µì§€ì‚¬í•­ ì •ë³´ ì¶”ì¶œ ë° ì¶œë ¥
#     for row in rows:
#         cols = row.find_all("td")
#         if len(cols) < 5:
#             continue  # êµ¬ì¡°ê°€ ë‹¤ë¥¸ ê²½ìš° ë¬´ì‹œ (ì˜ˆ: ê´‘ê³  row, ì˜ëª»ëœ row ë“±)

#         number = cols[0].text.strip()
#         subject_cell = cols[1]
#         link_tag = subject_cell.select_one("a")
#         title = link_tag.text.strip() if link_tag else subject_cell.text.strip()
#         href = link_tag["href"] if link_tag else ""
#         link = urljoin(BASE_URL, href)
#         writer = cols[2].text.strip()
#         date = cols[3].text.strip()
#         views = cols[4].text.strip()

#         print(f"[{number}] {title} / {writer} / {date} / ì¡°íšŒìˆ˜: {views}")
#         print(f"ğŸ”— ë§í¬: {link}")

#         all_data.append({
#             'number': number,
#             'title': title,
#             'writer': writer,
#             'date': date,
#             'views': views,
#             'link': link
#         })

# # 7. ë¸Œë¼ìš°ì € ë‹«ê¸°
# driver.quit()


# # â–¶ Spring APIë¡œ POST ì „ì†¡ ì‹œì˜ ì½”ë“œ

#Selenium + BeautifulSoup í˜¼í•© ì‚¬ìš©
import time
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# âœ… ì„¤ì •ê°’
MAX_PAGE = 10
BASE_URL = "https://eie.chosun.ac.kr"
LIST_URL = "https://eie.chosun.ac.kr/eie/5563/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGZWllJTJGMzIwJTJGYXJ0Y2xMaXN0LmRvJTNG"
API_URL = "http://localhost:6030/api/notices"

# âœ… Selenium ì„¤ì •
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰

# âœ… Chrome ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜ ë° ì‹¤í–‰
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

all_notices = []

# âœ… í˜ì´ì§€ ë°˜ë³µ í¬ë¡¤ë§
for page in range(1, MAX_PAGE + 1):
    print(f"\nâ–¶ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
    # ê° í˜ì´ì§€ì— ?page= íŒŒë¼ë¯¸í„° ë¶™ì—¬ ë°˜ë³µ
    driver.get(f"{LIST_URL}&page={page}")
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    # ìƒë‹¨ ê³ ì • ì œì™¸ (tr.notice ì œì™¸í•œ trë“¤ë§Œ í¬ë¡¤ë§)
    rows = soup.select("table.board-table.horizon5 tbody tr:not(.notice)")  

    print(f"í¬ë¡¤ë§í•œ ê³µì§€ ìˆ˜: {len(rows)}")

    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 5:
            continue

        number = cols[0].text.strip()
        subject_cell = cols[1]
        link_tag = subject_cell.select_one("a")
        title = link_tag.text.strip() if link_tag else subject_cell.text.strip()
        href = link_tag["href"] if link_tag else ""
        link = urljoin(BASE_URL, href)
        writer = cols[2].text.strip()
        date = cols[3].text.strip()
        views = cols[4].text.strip()

        print(f"[{number}] {title} / {writer} / {date} / ì¡°íšŒìˆ˜: {views}")
        print(f"ğŸ”— ë§í¬: {link}")

        notice = {
            'number': number,
            'title': title,
            'writer': writer,
            'date': date,
            'views': views,
            'link': link
        }
        all_notices.append(notice)

# âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ
driver.quit()

# âœ… Spring APIë¡œ POST ì „ì†¡ (requests.post())
headers = {"Content-Type": "application/json"}

# ì—ëŸ¬ ë¡œê·¸ì™€ ì„±ê³µ ë¡œê·¸ êµ¬ë¶„
for notice in all_notices:
    try:
        res = requests.post(API_URL, json=notice, headers=headers)
        if res.status_code == 200:
            print(f"âœ… ë“±ë¡ ì™„ë£Œ: {notice['title']}")
        else:
            print(f"âŒ ì‹¤íŒ¨: {res.status_code}, {res.text}")
    except Exception as e:
        print(f"ğŸš¨ ì˜ˆì™¸ ë°œìƒ: {e}")
