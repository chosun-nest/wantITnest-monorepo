# í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì˜ ì½”ë“œ
# (cmd -> $ cd ./crawler -> $ python notice_crawler.py)

# import time
# from urllib.parse import urljoin
# from bs4 import BeautifulSoup
# from selenium import webdriver
# from selenium.webdriver.chrome.service import Service
# from webdriver_manager.chrome import ChromeDriverManager

# # 1. Selenium ë“œë¼ì´ë²„ ì‹¤í–‰
# options = webdriver.ChromeOptions()
# options.add_argument("--headless")  # ì°½ ì—†ì´ ì‹¤í–‰í•˜ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ í•´ì œ
# # options.add_argument("--no-sandbox")
# # options.add_argument("--disable-dev-shm-usage")


# # í¬ë¡¬ ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜ ë° ì‹¤í–‰
# # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
# driver = webdriver.Chrome(options=options)

# # í˜ì´ì§€ ìˆ˜ ì„¤ì •
# MAX_PAGE = 10  # ì‹¤ì œ í•„ìš”í•œ ë§Œí¼ ì¡°ì •

# # 2. ì ‘ì†í•  URL (&page=)
# BASE_URL = "https://eie.chosun.ac.kr"
# LIST_URL = "https://eie.chosun.ac.kr/eie/5563/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGZWllJTJGMzIwJTJGYXJ0Y2xMaXN0LmRvJTNG"

# all_data = []

# for page in range(1, MAX_PAGE + 1):
#     print(f"â–¶ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

#     # í˜ì´ì§€ ì´ë™ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° êµ¬ì„±
#     url = f"{LIST_URL}&amp=&page={page}"
#     driver.get(url)

#     # 3. í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
#     time.sleep(2)

#     # 4. BeautifulSoupë¡œ íŒŒì‹±
#     soup = BeautifulSoup(driver.page_source, 'html.parser')

#     # 5. ê³µì§€ì‚¬í•­ í–‰ ì „ì²´ ê°€ì ¸ì˜¤ê¸° (ìƒë‹¨ê³ ì • í¬í•¨ ì¼ë°˜ê³µì§€ê¹Œì§€ ëª¨ë‘)
#     # rows = soup.select('table.board-table.horizon5 tbody tr')
#     # 5. ê³µì§€ì‚¬í•­ í–‰ ì „ì²´ ê°€ì ¸ì˜¤ê¸° (ìƒë‹¨ê³ ì • ì œì™¸í•œ ì¼ë°˜ê³µì§€ë§Œ)
#     rows = soup.select("table.board-table.horizon5 tbody tr:not(.notice)")

#     print(f"í¬ë¡¤ë§í•œ ê³µì§€ ìˆ˜: {len(rows)}")

#     # 6. ê³µì§€ì‚¬í•­ ì •ë³´ ì¶”ì¶œ ë° ì¶œë ¥
#     for row in rows:
#         cols = row.find_all("td")
#         if len(cols) < 5:
#             continue  # êµ¬ì¡°ê°€ ë‹¤ë¥¸ ê²½ìš° ë¬´ì‹œ (ì˜ˆ: ê´‘ê³  row, ì˜ëª»ëœ row ë“±)

#         number = cols[0].text.strip()
#         subject_cell = cols[1]
#         link_tag = subject_cell.select_one("a")
#         title = link_tag.text.strip() if link_tag else subject_cell.text.strip()
#         href = link_tag["href"] if link_tag else ""
#         link = urljoin(BASE_URL, href)
#         writer = cols[2].text.strip()
#         date = cols[3].text.strip()
#         views = cols[4].text.strip()

#         print(f"[{number}] {title} / {writer} / {date} / ì¡°íšŒìˆ˜: {views}")
#         print(f"ğŸ”— ë§í¬: {link}")

#         all_data.append({
#             'number': number,
#             'title': title,
#             'writer': writer,
#             'date': date,
#             'views': views,
#             'link': link
#         })

# # 7. ë¸Œë¼ìš°ì € ë‹«ê¸°
# driver.quit()

########
# # # â–¶ Spring APIë¡œ POST ì „ì†¡ ì‹œì˜ ì½”ë“œ

# #Selenium + BeautifulSoup í˜¼í•© ì‚¬ìš©
# import time
# import requests
# from urllib.parse import urljoin
# from bs4 import BeautifulSoup
# from selenium import webdriver
# from selenium.webdriver.chrome.service import Service
# from webdriver_manager.chrome import ChromeDriverManager

# # âœ… ì„¤ì •ê°’
# MAX_PAGE = 10
# BASE_URL = "https://eie.chosun.ac.kr"
# LIST_URL = "https://eie.chosun.ac.kr/eie/5563/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGZWllJTJGMzIwJTJGYXJ0Y2xMaXN0LmRvJTNG"
# API_URL = "http://localhost:6030/api/notices"

# # âœ… Selenium ì„¤ì •
# options = webdriver.ChromeOptions()
# options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰

# # âœ… Chrome ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜ ë° ì‹¤í–‰
# service = Service(ChromeDriverManager().install())
# driver = webdriver.Chrome(service=service, options=options)

# all_notices = []

# # âœ… í˜ì´ì§€ ë°˜ë³µ í¬ë¡¤ë§
# for page in range(1, MAX_PAGE + 1):
#     print(f"\nâ–¶ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
#     # ê° í˜ì´ì§€ì— ?page= íŒŒë¼ë¯¸í„° ë¶™ì—¬ ë°˜ë³µ
#     driver.get(f"{LIST_URL}&page={page}")
#     time.sleep(2)
#     soup = BeautifulSoup(driver.page_source, 'html.parser')
#     # ìƒë‹¨ ê³ ì • ì œì™¸ (tr.notice ì œì™¸í•œ trë“¤ë§Œ í¬ë¡¤ë§)
#     rows = soup.select("table.board-table.horizon5 tbody tr:not(.notice)")  

#     print(f"í¬ë¡¤ë§í•œ ê³µì§€ ìˆ˜: {len(rows)}")

#     for row in rows:
#         cols = row.find_all("td")
#         if len(cols) < 5:
#             continue

#         number = cols[0].text.strip()
#         subject_cell = cols[1]
#         link_tag = subject_cell.select_one("a")
#         title = link_tag.text.strip() if link_tag else subject_cell.text.strip()
#         href = link_tag["href"] if link_tag else ""
#         link = urljoin(BASE_URL, href)
#         writer = cols[2].text.strip()
#         date = cols[3].text.strip()
#         views = cols[4].text.strip()

#         print(f"[{number}] {title} / {writer} / {date} / ì¡°íšŒìˆ˜: {views}")
#         print(f"ğŸ”— ë§í¬: {link}")

#         notice = {
#             'number': number,
#             'title': title,
#             'writer': writer,
#             'date': date,
#             'views': views,
#             'link': link
#         }
#         all_notices.append(notice)

# # âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ
# driver.quit()

# # âœ… Spring APIë¡œ POST ì „ì†¡ (requests.post())
# headers = {"Content-Type": "application/json"}

# # ì—ëŸ¬ ë¡œê·¸ì™€ ì„±ê³µ ë¡œê·¸ êµ¬ë¶„
# for notice in all_notices:
#     try:
#         res = requests.post(API_URL, json=notice, headers=headers)
#         if res.status_code == 200:
#             print(f"âœ… ë“±ë¡ ì™„ë£Œ: {notice['title']}")
#         else:
#             print(f"âŒ ì‹¤íŒ¨: {res.status_code}, {res.text}")
#     except Exception as e:
#         print(f"ğŸš¨ ì˜ˆì™¸ ë°œìƒ: {e}")


########
## Python í¬ë¡¤ëŸ¬ë¥¼ FastAPI ì„œë²„ë¡œ ë°”ê¾¸ê¸°
# GET /crawlë¡œ ìš”ì²­í•˜ë©´ í•™ì‚¬ê³µì§€ í¬ë¡¤ë§ ë°ì´í„° JSONì„ ë°˜í™˜í•¨.

## FastAPI ì„œë²„ ì‹¤í–‰ ë°©ë²•
# uvicorn notice_crawler:app --host 0.0.0.0 --port 8000 --reload

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import requests
from apscheduler.schedulers.background import BackgroundScheduler  # âœ… ìŠ¤ì¼€ì¤„ëŸ¬ import ì¶”ê°€


app = FastAPI()

# âœ… CORS ì„¤ì • (React í†µì‹  í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš© ì „ì²´ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/crawl")
def crawl_notices():
    # âœ… í¬ë¡¤ë§ ëŒ€ìƒ URL
    LIST_URL = "https://eie.chosun.ac.kr/eie/5563/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGZWllJTJGMzIwJTJGYXJ0Y2xMaXN0LmRvJTNG"
    BASE_URL = "https://eie.chosun.ac.kr"

    # âœ… Selenium ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # ì°½ ì—†ì´ ì‹¤í–‰
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("user-agent=Mozilla/5.0")  # User-Agent ì¶”ê°€

    # âœ… ChromeDriver ìë™ ì„¤ì¹˜ ë° ë¡œë“œ
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    # âœ… í˜ì´ì§€ ì—´ê¸°
    driver.get(LIST_URL)
    time.sleep(2)  # ë¡œë”© ëŒ€ê¸° (JS ì‹¤í–‰ ì‹œê°„ í™•ë³´)

    # âœ… BeautifulSoup íŒŒì‹±
    soup = BeautifulSoup(driver.page_source, "html.parser")

    # âœ… í•„ìš”í•œ ë°ì´í„°ë§Œ í¬ë¡¤ë§
    rows = soup.select("table.board-table.horizon5 tbody tr:not(.notice)")

    notices = []

    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 5:
            continue  # ë°ì´í„° ë¶ˆì™„ì „í•œ í–‰ ë¬´ì‹œ

        number = cols[0].text.strip()
        title_cell = cols[1]
        title_tag = title_cell.select_one("a")
        title = title_tag.text.strip() if title_tag else title_cell.text.strip()
        link = BASE_URL + title_tag['href'] if title_tag else ""

        writer = cols[2].text.strip()
        date = cols[3].text.strip()
        views = cols[4].text.strip()

        notice = {
            "number": number,
            "title": title,
            "writer": writer,
            "date": date,
            "views": views,
            "link": link
        }
        notices.append(notice)

    # âœ… í¬ë¡¬ ë“œë¼ì´ë²„ ì¢…ë£Œ
    driver.quit()

    # âœ… í¬ë¡¤ë§ëœ ë°ì´í„° ë°˜í™˜
    return {"notices": notices}

# ì—¬ê¸°ì„œë¶€í„° ìƒˆë¡œìš´ í•¨ìˆ˜ ì¶”ê°€
SPRING_API_URL = "http://localhost:6030/api/notices"  # Spring API ì£¼ì†Œ

@app.get("/crawl-and-post")
def crawl_and_post_notices():
    notices = crawl_notices()["notices"]

    headers = {"Content-Type": "application/json"}

    success_count = 0
    fail_count = 0

    for notice in notices:
        try:
            res = requests.post(SPRING_API_URL, json=notice, headers=headers)
            if res.status_code == 200:
                success_count += 1
            else:
                fail_count += 1
        except Exception as e:
            print(f"ğŸš¨ ì˜ˆì™¸ ë°œìƒ: {e}")
            fail_count += 1

    return {"message": "ì „ì†¡ ì™„ë£Œ", "ì„±ê³µ": success_count, "ì‹¤íŒ¨": fail_count}

# âœ… (ì—¬ê¸°ë¶€í„°!!) ìŠ¤ì¼€ì¤„ëŸ¬ ì½”ë“œ ì¶”ê°€
def scheduled_crawl_and_post():
    print("â° í•˜ë£¨ 1ë²ˆ ìë™ í¬ë¡¤ë§ ë° ì „ì†¡ ì‹œì‘")
    crawl_and_post_notices()

scheduler = BackgroundScheduler()
scheduler.add_job(scheduled_crawl_and_post, "cron", hour=3, minute=0)  # ìƒˆë²½ 3ì‹œë§ˆë‹¤ ì‹¤í–‰
scheduler.start()